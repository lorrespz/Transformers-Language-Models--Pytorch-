{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPxu1cBjWGfgUnHarXlUWb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorrespz/Transformers-Language-Models--Pytorch-/blob/main/Transformer_Encoder_Decoder_Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoder architecture (Seq2seq) from scratch"
      ],
      "metadata": {
        "id": "OTG9Y0MGUoXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "y2EUXCxIUwy8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead Attention Block (with Causal Mask option)\n",
        "\n",
        "This block has an option to implement the causal mask (if set to True)"
      ],
      "metadata": {
        "id": "ImFUiQooU6Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, causal = False):\n",
        "    super().__init__()\n",
        "    #Assume d_v = d_k (len(Q) = len(K) = d_k, len(V) = d_v)\n",
        "    self.d_k = d_k\n",
        "    self.n_heads = n_heads\n",
        "    self.key = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.query = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.value = nn.Linear(d_model, d_k*n_heads)\n",
        "    #final linear layer\n",
        "    self.fc = nn.Linear(d_k*n_heads, d_model)\n",
        "\n",
        "    #causal mask: a square matrix of size max_len x max_len\n",
        "    #with the lower triangle half being all 1,\n",
        "    #upper triangle half being all 0\n",
        "    self.causal = causal\n",
        "    if causal:\n",
        "      cm = torch.tril(torch.ones(max_len, max_len))\n",
        "      self.register_buffer('causal_mask',\n",
        "                         cm.view(1,1,max_len, max_len))\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    q = self.query(q)   # N x T x (hd_k)\n",
        "    k = self.key(k)     # N x T x (hd_k)\n",
        "    v = self.value(v)    # N x T x (hd_v)\n",
        "\n",
        "    #h = n_heads\n",
        "    # N = batch size\n",
        "    N = q.shape[0]\n",
        "    # In seq2seq, the input and output\n",
        "    #sequences might have different lengths\n",
        "\n",
        "    # q comes from the decoder (output)\n",
        "    T_output = q.shape[1]\n",
        "    # k, v come from the encoder (input)\n",
        "    T_input = k.shape[1]\n",
        "\n",
        "    #change the shape to:\n",
        "    # (N, T, h, d_k) --> (N, h, T, d_k)\n",
        "    q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1,2)\n",
        "    k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1,2)\n",
        "    v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    #compute attention weights\n",
        "    # q * k^T\n",
        "    #(N,  h, T,  d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
        "    #transposing the last 2 dimensions of k\n",
        "    attn_scores = q @ k.transpose(-2, -1)/math.sqrt(self.d_k)\n",
        "    #apply the mask, which is a tensor of size (N,T) of values 0, 1\n",
        "    #for each of the N samples, need to know which of the T tokens is important\n",
        "    #Change from 2D to 4D by adding None, which introduces superfluous dim of size 1\n",
        "    # (N, T) --> (N, 1, 1, T)\n",
        "    if pad_mask is not None:\n",
        "      #mask_fill(arg1, arg2): if arg1 = True, apply arg2\n",
        "      #softmax(-inf) = 0\n",
        "       attn_scores = attn_scores.masked_fill(pad_mask[:, None, None,:] == 0, float('-inf'))\n",
        "       #HERE IS THE CAUSAL MASK !!!\n",
        "    if self.causal:\n",
        "       attn_scores = attn_scores.masked_fill(self.causal_mask[:, :, :T_output,:T_input] == 0, float('-inf'))\n",
        "    attn_weights = F.softmax(attn_scores, dim = -1)\n",
        "\n",
        "    #compute attention-weighted values\n",
        "    #(N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
        "    A = attn_weights @ v\n",
        "\n",
        "    #reshape it back before the final linear layer\n",
        "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
        "    A = A.contiguous().view(N, T_output, self.d_k*self.n_heads) #(N, T, h*d_k)\n",
        "\n",
        "    #final step is to project A with the Linear layer to\n",
        "    #get the same shape as the input sequence\n",
        "    return self.fc(A)"
      ],
      "metadata": {
        "id": "TH6Frf5fUxXq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Transformer Block\n",
        "\n",
        "This remains almost the same as the standalone encoder."
      ],
      "metadata": {
        "id": "MYv1HnjEU2KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the Transformer block of the Encoder\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*4, d_model),\n",
        "        nn.Dropout(dropout_prob)\n",
        "        )\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    #x is an input sequence of size (NxTXD)\n",
        "    # mask is of size (NxT)\n",
        "    #FIRST LAYER NORM:\n",
        "    #pass x in as the query, key, value into the multihead attention block\n",
        "    #then add the output to the residual 'x' to be passed in the 1st layer norm\n",
        "    x = self.ln1(x+ self.mha(x,x,x,mask))\n",
        "    # SECOND LAYER NORM: ann + x\n",
        "    x = self.ln2(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "G_gjoARBWQug"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Transformer Block\n",
        "\n",
        " This is the transformer block of the decoder. There are 2 multihead attention blocks:\n",
        " - One for the self attention within the decoder\n",
        " - One for the output of the encoder\n",
        " Structurally, it is very different from the transformer block of the standalone decoder in that it requires 2 multihead attention blocks and 3 LayerNorms (compared to 1 multihead attention and 2 LayerNorms)"
      ],
      "metadata": {
        "id": "RvbpqkQIWXcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.ln3 = nn.LayerNorm(d_model)\n",
        "    self.mha1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal = True)\n",
        "    self.mha2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal = False)\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*4, d_model),\n",
        "        nn.Dropout(dropout_prob)\n",
        "        )\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "\n",
        "  def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
        "    #self attention on the decoder input\n",
        "    x = self.ln1(dec_input+ self.mha1(dec_input, dec_input, dec_input, dec_mask))\n",
        "    # multihead attention including encoder output\n",
        "    x = self.ln2(x + self.mha2(x, enc_output, enc_output, enc_mask))\n",
        "    x = self.ln3(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "rnckWP8VXTA1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding Block"
      ],
      "metadata": {
        "id": "KkG0VH1nZXl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is exactly the same as the standalone encoder or decoder\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 2048, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "    #unsqueeze(1) adds a superfluous dim of size 1 at the end\n",
        "    #so that we have a 2d array of size (max_len, 1)\n",
        "    #position is pos variable in the formula\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    #exp_term is the '2i' in the exponent of the denominator in the formula\n",
        "    exp_term = torch.arange(0, d_model, 2)\n",
        "    #this is just the term 10000^(-2i/d_model)\n",
        "    div_term = torch.exp(exp_term*(-math.log(10000.0)/d_model))\n",
        "    #PE term\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    #0::2 means 2, 4, 6, 8, ... indexing\n",
        "    pe[0, :, 0::2] = torch.sin(position*div_term)\n",
        "    #1::2 means 1,3,5, 7, ... indexing\n",
        "    pe[0, :, 1::2] = torch.cos(position*div_term)\n",
        "    #register_buffer allows for saving and loading the model correctly\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x shape: NxTxD (D: d_model)\n",
        "    x  = x + self.pe[:,:x.size(1), :]\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "h7f4DkucZavv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder block"
      ],
      "metadata": {
        "id": "-1oC9TN_ZnYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size,\n",
        "               max_len, d_k, d_model, n_heads, n_layers,  dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformers_blocks = [EncoderBlock(d_k, d_model, n_heads, dropout_prob) for _ in range(n_layers)]\n",
        "    self.transformer_blocks = nn.Sequential(*transformers_blocks)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    #self.fc = nn.Linear(d_model, n_classes)\n",
        "\n",
        "  def forward(self, x, pad_mask = None):\n",
        "    x = self.embedding(x)\n",
        "    x = self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x, pad_mask)\n",
        "\n",
        "    #depends on the kind of tasks that we need, here:\n",
        "    #many-to-one (x has the shape N x T x D)\n",
        "    #x = x[:, 0, :]\n",
        "    x = self.ln(x)\n",
        "    #x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "6WmqIClUZuSb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder block"
      ],
      "metadata": {
        "id": "b-xLoleUZuhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformers_blocks = [DecoderBlock(d_k, d_model, n_heads, max_len, dropout_prob) for _ in range(n_layers)]\n",
        "    self.transformer_blocks = nn.Sequential(*transformers_blocks)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
        "    x = self.embedding(dec_input)\n",
        "    x = self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(enc_output, x, enc_mask, dec_mask)\n",
        "    x = self.ln(x)\n",
        "    x = self.fc(x) #many_to_many task\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "MB5NLzHZWWHo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Class with both Encoder & Decoder"
      ],
      "metadata": {
        "id": "sScx-ojUanqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
        "    enc_output = self.encoder(enc_input, enc_mask)\n",
        "    dec_output = self.decoder(enc_output, dec_input, enc_mask, dec_mask)\n",
        "    return dec_output\n"
      ],
      "metadata": {
        "id": "tTXWVCBxanzq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model with a dummy input\n",
        "\n",
        "Recall that this is a many-to-many model since the output of the model is text generation."
      ],
      "metadata": {
        "id": "hFavuPmCbMEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size = 20000,\n",
        "                  max_len = 512,\n",
        "                  d_k = 16,\n",
        "                  d_model = 64,\n",
        "                  n_heads = 4,\n",
        "                  n_layers = 2,\n",
        "                  dropout_prob = 0.1)\n",
        "\n",
        "decoder = Decoder(vocab_size = 10000,\n",
        "                  max_len = 512,\n",
        "                  d_k = 16,\n",
        "                  d_model = 64,\n",
        "                  n_heads = 4,\n",
        "                  n_layers = 2,\n",
        "                  dropout_prob = 0.1)\n",
        "transformer = Transformer(encoder, decoder)"
      ],
      "metadata": {
        "id": "ntzDnGQ7bJhm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNL9jnacbnyB",
        "outputId": "31922568-9b13-41dd-a724-45f1e605b58e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(10000, 64)\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): DecoderBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha1): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (mha2): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): DecoderBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha1): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (mha2): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=64, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the dummy inputs and masks\n",
        "\n",
        "#Encoder input: Batch size of 8, sequence length of 512\n",
        "x_en = np.random.randint(0, 20000, size = (8,512))\n",
        "x_ent = torch.tensor(x_en).to(device)\n",
        "# Decoder input: Batch size of 8, sequence length of 256\n",
        "x_dc = np.random.randint(0, 10000, size = (8,256))\n",
        "x_dct = torch.tensor(x_dc).to(device)\n",
        "\n",
        "#Masks\n",
        "maske = np.ones((8,512))\n",
        "maske[:,256:] = 0\n",
        "maske_t = torch.tensor(maske).to(device)\n",
        "\n",
        "maskd = np.ones((8,256))\n",
        "maskd[:,128:] = 0\n",
        "maskd_t = torch.tensor(maskd).to(device)\n"
      ],
      "metadata": {
        "id": "zD_3tNOlcNde"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder takes as inputs encoder input and encoder mask (see the forward method in the Encoder class)\n",
        "out1 = encoder(x_ent, maske_t)\n",
        "out1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn3pafAXbZpY",
        "outputId": "d05fb1ee-3b8d-4ec9-9196-91e81df5209e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder takes as inputs: Encoder output, Decoder input, Encoder mask, Decoder mask\n",
        "#forward method of decoder: (self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
        "out2 = decoder(out1, x_dct, maske_t, maskd_t)\n",
        "out2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgeNkYqpblfp",
        "outputId": "0072b8d8-3069-45a2-fb25-78ebf7c1eebf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 256, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = transformer(x_ent, x_dct, maske_t, maskd_t)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txAm8ntgbZrt",
        "outputId": "6420db94-ef97-43e2-d214-f4023f2e2de0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 256, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "id": "SJRmCDbxc0_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a72838-c6b5-4eed-fccb-5b05edf80514"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8787, -0.2507, -1.2625,  ..., -0.0921,  0.3364,  1.3597],\n",
              "         [ 0.0409,  0.1334,  0.3213,  ...,  0.3628, -0.1133,  0.7701],\n",
              "         [-0.4443,  0.8256, -0.1874,  ..., -0.2034,  0.3263,  0.7999],\n",
              "         ...,\n",
              "         [ 0.5611,  0.4037,  0.3164,  ..., -0.7057, -0.0457,  0.0446],\n",
              "         [ 0.3010,  0.1016, -0.3914,  ..., -0.3022,  0.1346, -0.5488],\n",
              "         [ 0.6013,  0.4361, -0.3773,  ...,  0.0678, -0.4806, -0.6275]],\n",
              "\n",
              "        [[ 0.0717,  0.4641, -0.6215,  ...,  1.0713,  0.0804,  0.4144],\n",
              "         [-0.4752,  0.5061, -1.3076,  ..., -0.5369,  1.0394,  1.0067],\n",
              "         [ 0.4456, -0.2973, -0.2378,  ...,  0.6716,  0.1164,  0.4415],\n",
              "         ...,\n",
              "         [ 0.6878,  0.5668,  0.2774,  ..., -0.3084,  0.6177,  0.7214],\n",
              "         [ 0.7308,  0.5590,  0.3725,  ...,  1.1489,  0.1562,  0.2182],\n",
              "         [ 1.0898,  0.4759, -0.4870,  ...,  0.1069,  0.7643,  0.2209]],\n",
              "\n",
              "        [[-0.1841, -0.5683,  0.5857,  ...,  1.2628,  0.7154, -0.3790],\n",
              "         [-0.9637, -0.0904,  0.5405,  ..., -0.0947, -0.6323,  0.7657],\n",
              "         [ 0.2272,  0.0061, -0.3202,  ...,  1.3210,  0.0651,  0.5609],\n",
              "         ...,\n",
              "         [ 0.0893,  0.5828, -0.3467,  ..., -0.9839,  1.1489, -0.2246],\n",
              "         [-0.0993,  0.4700, -0.3465,  ..., -0.4131, -0.9700,  0.9042],\n",
              "         [-0.0259,  0.1847, -0.6378,  ..., -0.2157, -0.5927, -0.2302]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.6214, -0.0131,  0.4086,  ...,  1.0479, -0.5937,  0.9848],\n",
              "         [-0.1180, -0.2324, -0.2290,  ...,  0.5324, -0.1169,  0.9464],\n",
              "         [ 0.3418, -0.2726, -0.0762,  ..., -0.3088,  1.2833,  1.5265],\n",
              "         ...,\n",
              "         [ 0.0934,  0.7030, -0.8423,  ..., -0.2833,  0.3152, -0.2530],\n",
              "         [ 0.4919,  0.1738,  0.2898,  ..., -0.5788,  0.0284, -0.9212],\n",
              "         [-0.8827,  0.9786,  0.2651,  ..., -0.2725,  0.1998, -0.0351]],\n",
              "\n",
              "        [[-0.2855,  0.3764, -0.6459,  ...,  0.4470, -0.0510,  1.7141],\n",
              "         [-0.4189, -0.7140, -0.4295,  ...,  0.2823, -0.4885,  0.0626],\n",
              "         [-0.7407, -0.7267, -0.1209,  ...,  1.0271,  0.3963,  0.6996],\n",
              "         ...,\n",
              "         [ 0.1292,  0.8938,  0.4397,  ..., -0.0928,  1.3315,  0.5587],\n",
              "         [ 0.6204,  0.1543, -0.3057,  ...,  0.3251,  0.4924, -0.3063],\n",
              "         [-0.0818,  0.9110,  0.3524,  ...,  0.7762,  0.2016, -0.1152]],\n",
              "\n",
              "        [[-0.9975, -0.5003, -0.4640,  ..., -0.4757,  0.5611,  0.7630],\n",
              "         [ 0.0575, -0.6209, -0.7485,  ...,  1.3250,  0.4835,  0.7426],\n",
              "         [ 0.1710, -0.1931,  0.6392,  ...,  0.0343, -0.2477, -0.0965],\n",
              "         ...,\n",
              "         [ 0.3201,  1.0226, -0.1798,  ...,  1.0229,  0.1596,  0.0064],\n",
              "         [-0.1106,  1.1648, -1.0890,  ...,  0.2051,  0.0684,  0.6024],\n",
              "         [-0.3025,  1.4296, -0.3249,  ...,  0.8576, -0.0983,  0.8398]]],\n",
              "       device='cuda:0', grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNVneHBaeyK6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}