{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZdz/6HsfPBS3cDaZWAIZR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorrespz/Transformers-Language-Models--Pytorch-/blob/main/Transformer_Encoder_Decoder_Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoder architecture (Seq2seq) from scratch"
      ],
      "metadata": {
        "id": "OTG9Y0MGUoXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "y2EUXCxIUwy8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead Attention Block (with Causal Mask option)\n",
        "\n",
        "This block has an option to implement the causal mask (if set to True)"
      ],
      "metadata": {
        "id": "ImFUiQooU6Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, causal = False):\n",
        "    super().__init__()\n",
        "    #Assume d_v = d_k (len(Q) = len(K) = d_k, len(V) = d_v)\n",
        "    self.d_k = d_k\n",
        "    self.n_heads = n_heads\n",
        "    self.key = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.query = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.value = nn.Linear(d_model, d_k*n_heads)\n",
        "    #final linear layer\n",
        "    self.fc = nn.Linear(d_k*n_heads, d_model)\n",
        "\n",
        "    #causal mask: a square matrix of size max_len x max_len\n",
        "    #with the lower triangle half being all 1,\n",
        "    #upper triangle half being all 0\n",
        "    self.causal = causal\n",
        "    if causal:\n",
        "      cm = torch.tril(torch.ones(max_len, max_len))\n",
        "      self.register_buffer('causal_mask',\n",
        "                         cm.view(1,1,max_len, max_len))\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    q = self.query(q)   # N x T x (hd_k)\n",
        "    k = self.key(k)     # N x T x (hd_k)\n",
        "    v = self.value(v)    # N x T x (hd_v)\n",
        "    #h = n_heads\n",
        "    # N = batch size\n",
        "    N = q.shape[0]\n",
        "    # In seq2seq, the input and output\n",
        "    #sequences might have different lengths\n",
        "    T_output = q.shape[1]\n",
        "    T_input = k.shape[1]\n",
        "\n",
        "    #change the shape to:\n",
        "    # (N, T, h, d_k) --> (N, h, T, d_k)\n",
        "    q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1,2)\n",
        "    k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1,2)\n",
        "    v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    #compute attention weights\n",
        "    # q * k^T\n",
        "    #(N,  h, T,  d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
        "    #transposing the last 2 dimensions of k\n",
        "    attn_scores = q @ k.transpose(-2, -1)/math.sqrt(self.d_k)\n",
        "    #apply the mask, which is a tensor of size (N,T) of values 0, 1\n",
        "    #for each of the N samples, need to know which of the T tokens is important\n",
        "    #Change from 2D to 4D by adding None, which introduces superfluous dim of size 1\n",
        "    # (N, T) --> (N, 1, 1, T)\n",
        "    if pad_mask is not None:\n",
        "      #mask_fill(arg1, arg2): if arg1 = True, apply arg2\n",
        "      #softmax(-inf) = 0\n",
        "       attn_scores = attn_scores.masked_fill(pad_mask[:, None, None,:] == 0, float('-inf'))\n",
        "       #HERE IS THE CAUSAL MASK !!!\n",
        "    if self.causal:\n",
        "       attn_scores = attn_scores.masked_fill(self.causal_mask[:, :, :T_output,:T_input] == 0, float('-inf'))\n",
        "    attn_weights = F.softmax(attn_scores, dim = -1)\n",
        "\n",
        "    #compute attention-weighted values\n",
        "    #(N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
        "    A = attn_weights @ v\n",
        "\n",
        "    #reshape it back before the final linear layer\n",
        "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
        "    A = A.contiguous().view(N, T_output, self.d_k*self.n_heads) #(N, T, h*d_k)\n",
        "\n",
        "    #final step is to project A with the Linear layer to\n",
        "    #get the same shape as the input sequence\n",
        "    return self.fc(A)"
      ],
      "metadata": {
        "id": "TH6Frf5fUxXq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Transformer Block\n",
        "\n",
        "This remains almost the same as the standalone encoder."
      ],
      "metadata": {
        "id": "MYv1HnjEU2KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the Transformer block of the Encoder\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*4, d_model),\n",
        "        nn.Dropout(dropout_prob)\n",
        "        )\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    #x is an input sequence of size (NxTXD)\n",
        "    # mask is of size (NxT)\n",
        "    #FIRST LAYER NORM:\n",
        "    #pass x in as the query, key, value into the multihead attention block\n",
        "    #then add the output to the residual 'x' to be passed in the 1st layer norm\n",
        "    x = self.ln1(x+ self.mha(x,x,x,mask))\n",
        "    # SECOND LAYER NORM: ann + x\n",
        "    x = self.ln2(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "G_gjoARBWQug"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Transformer Block\n",
        "\n",
        " This is the transformer block of the decoder. There are 2 multihead attention blocks:\n",
        " - One for the self attention within the decoder\n",
        " - One for the output of the encoder\n",
        " Structurally, it is very different from the transformer block of the standalone decoder in that it requires 2 multihead attention blocks and 3 LayerNorms (compared to 1 multihead attention and 2 LayerNorms)"
      ],
      "metadata": {
        "id": "RvbpqkQIWXcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.ln3 = nn.LayerNorm(d_model)\n",
        "    self.mha1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal = True)\n",
        "    self.mha2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal = False)\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*4, d_model),\n",
        "        nn.Dropout(dropout_prob)\n",
        "        )\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "\n",
        "  def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
        "    #self attention on the decoder input\n",
        "    x = self.ln1(dec_input+ self.mha1(dec_input, dec_input, dec_input, dec_mask))\n",
        "    # multihead attention including encoder output\n",
        "    x = self.ln2(x + self.mha2(x, enc_output, enc_output, enc_mask))\n",
        "    x = self.ln3(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "rnckWP8VXTA1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding Block"
      ],
      "metadata": {
        "id": "KkG0VH1nZXl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is exactly the same as the standalone encoder or decoder\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 2048, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "    #unsqueeze(1) adds a superfluous dim of size 1 at the end\n",
        "    #so that we have a 2d array of size (max_len, 1)\n",
        "    #position is pos variable in the formula\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    #exp_term is the '2i' in the exponent of the denominator in the formula\n",
        "    exp_term = torch.arange(0, d_model, 2)\n",
        "    #this is just the term 10000^(-2i/d_model)\n",
        "    div_term = torch.exp(exp_term*(-math.log(10000.0)/d_model))\n",
        "    #PE term\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    #0::2 means 2, 4, 6, 8, ... indexing\n",
        "    pe[0, :, 0::2] = torch.sin(position*div_term)\n",
        "    #1::2 means 1,3,5, 7, ... indexing\n",
        "    pe[0, :, 1::2] = torch.cos(position*div_term)\n",
        "    #register_buffer allows for saving and loading the model correctly\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x shape: NxTxD (D: d_model)\n",
        "    x  = x + self.pe[:,:x.size(1), :]\n",
        "    return self.dropout(x)\n",
        ""
      ],
      "metadata": {
        "id": "h7f4DkucZavv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder block"
      ],
      "metadata": {
        "id": "-1oC9TN_ZnYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size,\n",
        "               max_len, d_k, d_model, n_heads, n_layers,  dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformers_blocks = [EncoderBlock(d_k, d_model, n_heads, dropout_prob) for _ in range(n_layers)]\n",
        "    self.transformer_blocks = nn.Sequential(*transformers_blocks)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    #self.fc = nn.Linear(d_model, n_classes)\n",
        "\n",
        "  def forward(self, x, pad_mask = None):\n",
        "    x = self.embedding(x)\n",
        "    x = self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x, pad_mask)\n",
        "\n",
        "    #depends on the kind of tasks that we need, here:\n",
        "    #many-to-one (x has the shape N x T x D)\n",
        "    #x = x[:, 0, :]\n",
        "    x = self.ln(x)\n",
        "    #x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "6WmqIClUZuSb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder block"
      ],
      "metadata": {
        "id": "b-xLoleUZuhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformers_blocks = [DecoderBlock(d_k, d_model, n_heads, max_len, dropout_prob) for _ in range(n_layers)]\n",
        "    self.transformer_blocks = nn.Sequential(*transformers_blocks)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
        "    x = self.embedding(dec_input)\n",
        "    x = self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(enc_output, x, enc_mask, dec_mask)\n",
        "    x = self.ln(x)\n",
        "    x = self.fc(x) #many_to_many task\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "MB5NLzHZWWHo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Class with both Encoder & Decoder"
      ],
      "metadata": {
        "id": "sScx-ojUanqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
        "    enc_output = self.encoder(enc_input, enc_mask)\n",
        "    dec_output = self.decoder(enc_output, dec_input, enc_mask, dec_mask)\n",
        "    return dec_output\n",
        ""
      ],
      "metadata": {
        "id": "tTXWVCBxanzq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model with a dummy input"
      ],
      "metadata": {
        "id": "hFavuPmCbMEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size = 20000,\n",
        "                  max_len = 512,\n",
        "                  d_k = 16,\n",
        "                  d_model = 64,\n",
        "                  n_heads = 4,\n",
        "                  n_layers = 2,\n",
        "                  dropout_prob = 0.1)\n",
        "\n",
        "decoder = Decoder(vocab_size = 10000,\n",
        "                  max_len = 512,\n",
        "                  d_k = 16,\n",
        "                  d_model = 64,\n",
        "                  n_heads = 4,\n",
        "                  n_layers = 2,\n",
        "                  dropout_prob = 0.1)\n",
        "transformer = Transformer(encoder, decoder)"
      ],
      "metadata": {
        "id": "ntzDnGQ7bJhm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNL9jnacbnyB",
        "outputId": "fc56ff0a-6b70-49e3-f941-b200a10f1c56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(10000, 64)\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): DecoderBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha1): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (mha2): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): DecoderBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha1): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (mha2): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=64, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_en = np.random.randint(0, 20000, size = (8,512))\n",
        "x_ent = torch.tensor(x_en).to(device)\n",
        "\n",
        "x_dc = np.random.randint(0, 10000, size = (8,512))\n",
        "x_dct = torch.tensor(x_dc).to(device)\n",
        "\n",
        "#Masks\n",
        "maske = np.ones((8,512))\n",
        "maske[:,256:] = 0\n",
        "maske_t = torch.tensor(maske).to(device)\n",
        "\n",
        "maskd = np.ones((8,256))\n",
        "maskd[:,128:] = 0\n",
        "maskd_t = torch.tensor(maskd).to(device)\n",
        "\n",
        "#out = transformer(x_ent, x_dct, maske_t, maskd_t)\n",
        "#out.shape\n"
      ],
      "metadata": {
        "id": "zD_3tNOlcNde"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE CONTINUED"
      ],
      "metadata": {
        "id": "SJRmCDbxc0_g"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}