{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMISfBb87UPi3cWO92JNQCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorrespz/Transformers-Language-Models--Pytorch-/blob/main/Transformers_Decoder_(GPT_like)_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder architecture (GPT-like model) from scratch\n",
        "\n",
        "This code is from Lazy Programmer's Transformers course\n",
        "\n",
        "https://www.udemy.com/course/data-science-transformers-nlp/\n",
        "\n",
        "Recall that GPT models are a family of language models in which the main pretraining task is performed by predicting a token using **only** the tokens that come before it. In short, the prediction is for the probability $p_t$\n",
        "\n",
        "$\\langle p_t\\,| \\ldots, p_{t-2}, p_{t-1}\\rangle$\n",
        "\n",
        "Decoder architecture is largely similar to encoder architecture, with the only difference being the attention block used is the causal version (which implements the dependency solely on past tokens)."
      ],
      "metadata": {
        "id": "7YA3AmH0whfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9ggE9jIHwuzt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Attention Block\n",
        "\n",
        "The structure of the causal attention block is almost identical to that of the normal attention block used in Encoder Language Model, with the exception being the presence of a 'causal attention mask' applied before the softmax in the attention formula to set all past tokens to 1."
      ],
      "metadata": {
        "id": "TdS0QG3dxc4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len):\n",
        "    super().__init__()\n",
        "    #Assume d_v = d_k (len(Q) = len(K) = d_k, len(V) = d_v)\n",
        "    self.d_k = d_k\n",
        "    self.n_heads = n_heads\n",
        "    self.key = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.query = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.value = nn.Linear(d_model, d_k*n_heads)\n",
        "    #final linear layer\n",
        "    self.fc = nn.Linear(d_k*n_heads, d_model)\n",
        "    #causal mask: a square matrix of size max_len x max_len\n",
        "    #with the lower triangle half being all 1,\n",
        "    #upper triangle half being all 0\n",
        "    cm = torch.tril(torch.ones(max_len, max_len))\n",
        "    self.register_buffer('causal_mask',\n",
        "                         cm.view(1,1,max_len, max_len))\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask = None):\n",
        "    q = self.query(q)   # N x T x (hd_k)\n",
        "    k = self.key(k)     # N x T x (hd_k)\n",
        "    v = self.value(v)    # N x T x (hd_v)\n",
        "    #h = n_heads\n",
        "    # N = batch size\n",
        "    N = q.shape[0]\n",
        "    # T = sequence length\n",
        "    T = q.shape[1]\n",
        "\n",
        "    #change the shape to:\n",
        "    # (N, T, h, d_k) --> (N, h, T, d_k)\n",
        "    q = q.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "    k = k.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "    v = v.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    #compute attention weights\n",
        "    # q * k^T\n",
        "    #(N,  h, T,  d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
        "    #transposing the last 2 dimensions of k\n",
        "    attn_scores = q @ k.transpose(-2, -1)/math.sqrt(self.d_k)\n",
        "    #apply the mask, which is a tensor of size (N,T) of values 0, 1\n",
        "    #for each of the N samples, need to know which of the T tokens is important\n",
        "    #Change from 2D to 4D by adding None, which introduces superfluous dim of size 1\n",
        "    # (N, T) --> (N, 1, 1, T)\n",
        "    if pad_mask is not None:\n",
        "      #mask_fill(arg1, arg2): if arg1 = True, apply arg2\n",
        "      #softmax(-inf) = 0\n",
        "       attn_scores = attn_scores.masked_fill(pad_mask[:, None, None,:] == 0, float('-inf'))\n",
        "       #HERE IS THE CAUSAL MASK !!!\n",
        "       attn_scores = attn_scores.masked_fill(self.causal_mask[:, :, :T,:T] == 0, float('-inf'))\n",
        "    attn_weights = F.softmax(attn_scores, dim = -1)\n",
        "\n",
        "    #compute attention-weighted values\n",
        "    #(N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
        "    A = attn_weights @ v\n",
        "\n",
        "    #reshape it back before the final linear layer\n",
        "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
        "    A = A.contiguous().view(N, T, self.d_k*self.n_heads) #(N, T, h*d_k)\n",
        "\n",
        "    #final step is to project A with the Linear layer to\n",
        "    #get the same shape as the input sequence\n",
        "    return self.fc(A)\n"
      ],
      "metadata": {
        "id": "VKwRijsMxD4o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block\n",
        "\n",
        "Almost the same as that used in the Encoder architecture, with the only difference being an additional input 'max_len'"
      ],
      "metadata": {
        "id": "EjcSjaw7zRob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.mha = CausalSelfAttention(d_k, d_model, n_heads, max_len)\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*4, d_model),\n",
        "        nn.Dropout(dropout_prob)\n",
        "        )\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "\n",
        "  def forward(self, x, pad_mask = None):\n",
        "    #x is an input sequence of size (NxTXD)\n",
        "    # mask is of size (NxT)\n",
        "    #FIRST LAYER NORM:\n",
        "    #pass x in as the query, key, value into the multihead attention block\n",
        "    #then add the output to the residual 'x' to be passed in the 1st layer norm\n",
        "    x = self.ln1(x+ self.mha(x,x,x,pad_mask))\n",
        "    # SECOND LAYER NORM: ann + x\n",
        "    x = self.ln2(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "Z01ylYbRzZJ3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding Block"
      ],
      "metadata": {
        "id": "pwrrvg84z3sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 2048, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "    #unsqueeze(1) adds a superfluous dim of size 1 at the end\n",
        "    #so that we have a 2d array of size (max_len, 1)\n",
        "    #position is pos variable in the formula\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    #exp_term is the '2i' in the exponent of the denominator in the formula\n",
        "    exp_term = torch.arange(0, d_model, 2)\n",
        "    #this is just the term 10000^(-2i/d_model)\n",
        "    div_term = torch.exp(exp_term*(-math.log(10000.0)/d_model))\n",
        "    #PE term\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    #0::2 means 2, 4, 6, 8, ... indexing\n",
        "    pe[0, :, 0::2] = torch.sin(position*div_term)\n",
        "    #1::2 means 1,3,5, 7, ... indexing\n",
        "    pe[0, :, 1::2] = torch.cos(position*div_term)\n",
        "    #register_buffer allows for saving and loading the model correctly\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x shape: NxTxD (D: d_model)\n",
        "    x  = x + self.pe[:,:x.size(1), :]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "MsNwc83vz3If"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Block"
      ],
      "metadata": {
        "id": "1xgmiskmz-hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformers_blocks = [TransformerBlock(d_k, d_model, n_heads, max_len, dropout_prob) for _ in range(n_layers)]\n",
        "    self.transformer_blocks = nn.Sequential(*transformers_blocks)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x, pad_mask = None):\n",
        "    x = self.embedding(x)\n",
        "    x = self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x, pad_mask)\n",
        "    x = self.ln(x)\n",
        "    x = self.fc(x) #many_to_many tasl\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "KZRffmlg0Amk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the decoder with a random input"
      ],
      "metadata": {
        "id": "xqatlR9H1YH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(20000, 1024, 16, 64, 4, 2, 0.1)"
      ],
      "metadata": {
        "id": "EJVqklLj1WvW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_LGD_y_1iVa",
        "outputId": "3464f215-6fc7-4a73-fdca-a87b8652f178"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(20000, 64)\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=64, out_features=20000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some random sequence of integers representing a sequence of tokens of size (N,T) =(8, 512)\n",
        "x = np.random.randint(0, 20000, size = (8,512))\n",
        "x_t = torch.tensor(x).to(device)\n",
        "x_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M02gBYN92AUS",
        "outputId": "777e2a1d-046b-404f-b270-94f1ea9fa5c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6043, 10930,   177,  ...,   681, 10249,  8482],\n",
              "        [ 4863,  1640, 16227,  ...,  2034,  7062, 13644],\n",
              "        [11400,  7823,  1907,  ..., 18434,   898, 11997],\n",
              "        ...,\n",
              "        [ 5067, 19949,  6301,  ..., 17548, 17588,   612],\n",
              "        [16764,  4361, 17452,  ..., 13244,  6946,   113],\n",
              "        [14127, 15075, 19763,  ...,  6535,  6059, 17018]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the corresponding mask for the input above\n",
        "mask = np.ones((8,512))\n",
        "mask[:, 256:] = 0\n",
        "mask_t = torch.tensor(mask).to(device)\n",
        "mask_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkubka6T2FRr",
        "outputId": "db363007-61ca-4df7-a7e7-967feb523b36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the input and mask thru the encoder\n",
        "y = model(x_t, mask_t)\n",
        "#The shape should be (8, 512, vocab_size = 20000)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kKrEGtR2IkA",
        "outputId": "c66cce43-bc0a-42df-fb2a-f7296c7107f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 20000])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EcK0ABF22MSI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}