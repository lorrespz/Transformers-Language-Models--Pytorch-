{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiQ6ZEAlFNkmJFJdY+YyFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorrespz/Transformers-Language-Models-Pytorch-implementation/blob/main/Transformers_Pytorch_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XzPV7-hQF46A"
      },
      "outputs": [],
      "source": [
        "# Lazy Programmer's Transformers course\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead Attention Block"
      ],
      "metadata": {
        "id": "OVcb6NQcTLxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the formula:\n",
        "\n",
        "   Attention($Q, K, D$) = softmax$\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "\n",
        "  where:\n",
        "\n",
        "   $ Q = W^Q Q_{input}$\n",
        "\n",
        "   $ K = W^K K_{input}$\n",
        "\n",
        "   $V = W^V V_{input}$\n"
      ],
      "metadata": {
        "id": "d7MpXEkgJGNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d, k, d_model, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    #Assume d_v = d_k (len(Q) = len(K) = d_k, len(V) = d_v)\n",
        "    self.d_k = d_k\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    self.key = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.query = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.value = nn.Linear(d_model, d_k*n_heads)\n",
        "\n",
        "    #final linear layer\n",
        "    self.fc = nn.Linear(d_k*n_heads, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask = None):\n",
        "      q = self.query(q)   # N x T x (hd_k)\n",
        "      k = self.key(k)     # N x T x (hd_k)\n",
        "      v = self.value(v)     # N x T x (hd_v)\n",
        "      #h = n_heads\n",
        "\n",
        "      # N = batch size\n",
        "      N = q.shape[0]\n",
        "      # T = sequence length\n",
        "      T = q.shape[1]\n",
        "\n",
        "      #change the shape to:\n",
        "      # (N, T, h, d_k) --> (N, h, T, d_k)\n",
        "      q = q.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "      k = k.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "      v = v.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "      #compute attention weights\n",
        "      # q * k^T\n",
        "      #(N,  h, T,  d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
        "      #transposing the last 2 dimensions of k\n",
        "      attn_scores = q @ k.transpose(-2, -1)/math.sqrt(self.d_k)\n",
        "      #apply the mask, which is a tensor of size (N,T) of values 0, 1\n",
        "      #for each of the N samples, need to know which of the T tokens is important\n",
        "      #Change from 2D to 4D by adding None, which introduces superfluous dim of size 1\n",
        "      # (N, T) --> (N, 1, 1, T)\n",
        "      if mask is not None:\n",
        "        #mask_fill(arg1, arg2): if arg1 = True, apply arg2\n",
        "        #softmax(-inf) = 0\n",
        "        attn_scores = attn_scores.masked_fill(mask[:, None, None,:] == 0, float('-inf'))\n",
        "      attn_weights = F.softmax(attn_scores, dim = -1)\n",
        "\n",
        "      #compute attention-weighted values\n",
        "      #(N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
        "      A = attn_weights @ v\n",
        "\n",
        "      #reshape it back before the final linear layer\n",
        "      A = A.transpose(1, 2) # (N, T, h, d_k)\n",
        "      A = A.contiguous().view(N, T, self.d_k*self.n_heads) #(N, T, h*d_k)\n",
        "\n",
        "      #final step is to project A with the Linear layer to\n",
        "      #get the same shape as the input sequence\n",
        "      return self.fc(A)\n"
      ],
      "metadata": {
        "id": "XHHu-OGyGQ49"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block"
      ],
      "metadata": {
        "id": "cMXw2AfnJFWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.mha = MultiHeadAttention(d_k, d_model, n_heads)\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*4, d_model),\n",
        "        nn.Dropout(dropout_prob)\n",
        "        )\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "      #x is an input sequence of size (NxTXD)\n",
        "      # mask is of size (NxT)\n",
        "      #FIRST LAYER NORM:\n",
        "      #pass x in as the query, key, value into the multihead attention block\n",
        "      #then add the output to the residual 'x' to be passed in the 1st layer norm\n",
        "      x = self.ln1(x+ self.mha(x,x,x,mask))\n",
        "      # SECOND LAYER NORM: ann + x\n",
        "      x = self.ln2(x + self.ann(x))\n",
        "      x = self.dropout(x)\n",
        "      return(x)"
      ],
      "metadata": {
        "id": "dTQPwC3lTFvY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding Block\n",
        "\n",
        "\n",
        "$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$\n",
        "\n",
        "$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$"
      ],
      "metadata": {
        "id": "KhiYW5FqW51n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 2048, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "    #unsqueeze(1) adds a superfluous dim of size 1 at the end\n",
        "    #so that we have a 2d array of size (max_len, 1)\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    exp_term = torch.arange(0, d_model, 2)\n",
        "    div_term = torch.exp(exp_term*(-math.log(10000.0)/d_model))\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    pe[0, :, 0::2] = torch.sin(position*div_term)\n",
        "    pe[0, :, 1::2] = torch.cos(position*div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #x shape: NxTxD\n",
        "      x  = x + self.pe[:,:,x.size(1), :]\n",
        "      return self.dropout(x)"
      ],
      "metadata": {
        "id": "tHJhEsgMVL3n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qo9GHUcfZiZe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}